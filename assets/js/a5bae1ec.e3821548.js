"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([[263180],{15680:(e,n,t)=>{t.d(n,{xA:()=>s,yg:()=>f});var o=t(296540);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function c(e,n){if(null==e)return{};var t,o,r=function(e,n){if(null==e)return{};var t,o,r={},i=Object.keys(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=o.createContext({}),p=function(e){var n=o.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},s=function(e){var n=p(e.components);return o.createElement(l.Provider,{value:n},e.children)},u="mdxType",g={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},d=o.forwardRef((function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,s=c(e,["components","mdxType","originalType","parentName"]),u=p(t),d=r,f=u["".concat(l,".").concat(d)]||u[d]||g[d]||i;return t?o.createElement(f,a(a({ref:n},s),{},{components:t})):o.createElement(f,a({ref:n},s))}));function f(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,a=new Array(i);a[0]=d;var c={};for(var l in n)hasOwnProperty.call(n,l)&&(c[l]=n[l]);c.originalType=e,c[u]="string"==typeof e?e:r,a[1]=c;for(var p=2;p<i;p++)a[p]=t[p];return o.createElement.apply(null,a)}return o.createElement.apply(null,t)}d.displayName="MDXCreateElement"},768954:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>g,frontMatter:()=>i,metadata:()=>c,toc:()=>p});var o=t(58168),r=(t(296540),t(15680));const i={title:"How to access a new Trino Connector plugin",language:"en"},a=void 0,c={unversionedId:"how-to-contribute/trino-connector-developer-guide",id:"how-to-contribute/trino-connector-developer-guide",title:"How to access a new Trino Connector plugin",description:"\x3c!--",source:"@site/community/how-to-contribute/trino-connector-developer-guide.md",sourceDirName:"how-to-contribute",slug:"/how-to-contribute/trino-connector-developer-guide",permalink:"/community/how-to-contribute/trino-connector-developer-guide",draft:!1,tags:[],version:"current",frontMatter:{title:"How to access a new Trino Connector plugin",language:"en"},sidebar:"community",previous:{title:"How to Share Blogs",permalink:"/community/how-to-contribute/how-to-share-blogs"},next:{title:"Doris Versioning",permalink:"/community/release-versioning"}},l={},p=[{value:"Background",id:"background",level:2},{value:"Step 1: Compile Kakfa connector plugin",id:"step-1-compile-kakfa-connector-plugin",level:2},{value:"Step 2: Set up Doris&#39;s fe.conf / be.conf",id:"step-2-set-up-doriss-feconf--beconf",level:2},{value:"Step 3: Using the Trino-Connector catalog feature",id:"step-3-using-the-trino-connector-catalog-feature",level:2}],s={toc:p},u="wrapper";function g(e){let{components:n,...t}=e;return(0,r.yg)(u,(0,o.A)({},s,t,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"how-to-access-a-new-trino-connector-plugin"},"How to access a new Trino Connector plugin"),(0,r.yg)("h2",{id:"background"},"Background"),(0,r.yg)("p",null,"Starting from version 3.0, Doris supports docking with the Trino Connector plugin. Through the rich Trino Connector plugin and Doris' Trino-Connector Catalog function, Doris can query more data sources."),(0,r.yg)("p",null,"The purpose of the Trino Connector compatibility framework is to help Doris quickly connect to more data sources to meet user needs."),(0,r.yg)("p",null,"For data sources such as Hive, Iceberg, Hudi, Paimon, and JDBC, we still recommend using the built-in catalog of Doris  for connection to achieve better performance, stability, and compatibility."),(0,r.yg)("p",null,"This article mainly introduces how to adapt a Trino Connector plugin in Doris."),(0,r.yg)("p",null,"The following takes Trino's kafka Connector plugin as an example to introduce in detail how to adapt Trino's kafka Connector plugin in Doris, and then access the kafka data source through the ",(0,r.yg)("inlineCode",{parentName:"p"},"Trino-Connector")," catalog function of Doris."),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Note: Trino is an Apache License 2.0 protocol open source software provided by ",(0,r.yg)("a",{parentName:"p",href:"https://trino.io/foundation"},"Trino Software Foundation"),". For details, please visit ",(0,r.yg)("a",{parentName:"p",href:"https://trino.io/docs/current/"},"Trino official website"),".")),(0,r.yg)("h2",{id:"step-1-compile-kakfa-connector-plugin"},"Step 1: Compile Kakfa connector plugin"),(0,r.yg)("p",null,"Trino does not provide officially compiled connector plugins, so we need to compile the required connector plugins ourselves."),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Note: Since Doris currently uses the 435 version of the ",(0,r.yg)("inlineCode",{parentName:"p"},"trino-main")," package, it is best to compile the 435 version of the connector plugin. There may be compatibility issues with non-435 versions of the connector plugin. If you encounter any problems, please provide feedback to the Apache Doris community.")),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Clone Trino source code\n",(0,r.yg)("inlineCode",{parentName:"li"},"$ git clone https://github.com/trinodb/trino.git")),(0,r.yg)("li",{parentName:"ol"},"Switch Trino source code to version 435\n",(0,r.yg)("inlineCode",{parentName:"li"},"$ git checkout 435")),(0,r.yg)("li",{parentName:"ol"},"Enter the Kafka plugin source code directory\n",(0,r.yg)("inlineCode",{parentName:"li"},"$ cd trino/plugin/trino-kafka")),(0,r.yg)("li",{parentName:"ol"},"Compile the Kafka plugin\n",(0,r.yg)("inlineCode",{parentName:"li"},"$ mvn clean install -DskipTest")),(0,r.yg)("li",{parentName:"ol"},"After the compilation is completed, the target/trino-kafka-435 directory will be generated in the trino/plugin/trino-kafka/ directory.")),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Note: Each connector plugin is a subdirectory, not a jar package.")),(0,r.yg)("h2",{id:"step-2-set-up-doriss-feconf--beconf"},"Step 2: Set up Doris's fe.conf / be.conf"),(0,r.yg)("p",null,"After preparing the Kafka connector plug-in, you need to configure Doris's fe.conf and be.conf so that Doris can load the plug-in."),(0,r.yg)("p",null,"If we store the ",(0,r.yg)("inlineCode",{parentName:"p"},"trino-kafka-435")," directory prepared above in the /path/to/connectors directory, and then we should configure:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"fe.conf"),(0,r.yg)("p",{parentName:"li"},"Configure ",(0,r.yg)("inlineCode",{parentName:"p"},"trino_connector_plugin_dir=/path/to/connectors")," in the fe.conf file (if the ",(0,r.yg)("inlineCode",{parentName:"p"},"trino_connector_plugin_dir")," attribute is not configured in fe.conf, the ",(0,r.yg)("inlineCode",{parentName:"p"},"${Doris_HOME}/fe/connectors")," directory will be used by default).")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"be.conf"),(0,r.yg)("p",{parentName:"li"},"Configure ",(0,r.yg)("inlineCode",{parentName:"p"},"trino_connector_plugin_dir=/path/to/connectors")," in the be.conf file (if the ",(0,r.yg)("inlineCode",{parentName:"p"},"trino_connector_plugin_dir")," attribute is not configured in be.conf, the ",(0,r.yg)("inlineCode",{parentName:"p"},"${Doris_HOME}/be/connectors")," directory will be used by default)."))),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Note: Doris uses a lazy loading method to load the Trino Connector plug-in, which means that if it is the first time to use the Trino-Connector Catalog function in Doris, there is no need to restart the FE / BE node, Doris will automatically load the plug-in. However, the plug-in will only be loaded once, so if the plug-in in the ",(0,r.yg)("inlineCode",{parentName:"p"},"/path/to/connectors/")," directory changes, you need to restart the FE / BE node before the changed plug-in can be loaded.")),(0,r.yg)("h2",{id:"step-3-using-the-trino-connector-catalog-feature"},"Step 3: Using the Trino-Connector catalog feature"),(0,r.yg)("p",null,"After completing the previous two steps, we can use the Trino-Connector Catalog function in Doris."),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"First let's create a Trino-Connector Catalog in Doris:"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'create catalog kafka_tpch properties (\n    "type"="trino-connector",\n    -- The following four properties are derived from trino and are consistent with the properties in etc/catalog/kakfa.properties of trino. But need to add "trino." prefix\n    "trino.connector.name"="kafka",\n    "trino.kafka.table-names"="tpch.customer,tpch.orders,tpch.lineitem,tpch.part,tpch.partsupp,tpch.supplier,tpch.nation,tpch.region",\n    "trino.kafka.nodes"="localhost:9092",\n    "trino.kafka.table-description-dir" = "/mnt/datadisk1/fangtiewei"\n);\n')),(0,r.yg)("p",{parentName:"li"},"explain:"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"type")," \uff1aThe type of catalog, here we must set it to ",(0,r.yg)("inlineCode",{parentName:"li"},"trino-connector"),"."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"trino.connector.name"),"\u3001",(0,r.yg)("inlineCode",{parentName:"li"},"trino.kafka.table-names"),"\u3001",(0,r.yg)("inlineCode",{parentName:"li"},"trino.kafka.nodes"),"\u3001",(0,r.yg)("inlineCode",{parentName:"li"},"trino.kafka.table-description-dir")," The following four properties are derived from trino, refer to: ",(0,r.yg)("a",{parentName:"li",href:"https://trino.io/docs/current/connector/kafka.html#configuration"},"Kafka connector"))),(0,r.yg)("p",{parentName:"li"},"Different Connector plug-ins should set different properties. You can refer to the official trino documentation: ",(0,r.yg)("a",{parentName:"p",href:"https://trino.io/docs/current/connector.html#connector--page-root"},"Connectors"))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Use catalog"),(0,r.yg)("p",{parentName:"li"},"After we create the Trino-Connector catalog, there is no difference in use from other catalogs. Switch to the catalog through the ",(0,r.yg)("inlineCode",{parentName:"p"},"switch kafka_tpch")," statement, and then you can query the data of the Kafka data source."))),(0,r.yg)("p",null,"The following are the Doris Trino-Connector catalog configuration of several commonly used Connector plug-ins."),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Hive"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'create catalog emr_hive properties (\n    "type"="trino-connector",\n    "trino.connector.name"="hive",\n    "trino.hive.metastore.uri"="thrift://ip:port",\n    "trino.hive.config.resources"="/path/to/core-site.xml,/path/to/hdfs-site.xml"\n);\n')),(0,r.yg)("blockquote",{parentName:"li"},(0,r.yg)("p",{parentName:"blockquote"},"Note:"),(0,r.yg)("ul",{parentName:"blockquote"},(0,r.yg)("li",{parentName:"ul"},'You should add Hadoop\'s user name in the JVM parameters: -DHADOOP_USER_NAME=user, which can be configured at the end of the JAVA_OPTS_FOR_JDK_17 parameter in the fe.conf / be.conf file, such as JAVA_OPTS_FOR_JDK_17="...-DHADOOP_USER_NAME=user"')))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Mysql"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'create catalog trino_mysql properties (\n    "type"="trino-connector",\n    "trino.connector.name"="mysql",\n    "trino.connection-url" = "jdbc:mysql://ip:port",\n    "trino.connection-user" = "user",\n    "trino.connection-password" = "password"\n);\n')),(0,r.yg)("blockquote",{parentName:"li"},(0,r.yg)("p",{parentName:"blockquote"},"Note:"),(0,r.yg)("ul",{parentName:"blockquote"},(0,r.yg)("li",{parentName:"ul"},"When encountering the error: Unknown or incorrect time zone: 'Asia/Shanghai', you need to add: -Duser.timezone=Etc/GMT-8 to the JVM startup parameters, which can be configured at the end of the JAVA_OPTS_FOR_JDK_17 parameter in the fe.conf / be.conf file.")))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Kafka"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'create catalog kafka properties (\n    "type"="trino-connector",\n    "trino.connector.name"="kafka",\n    "trino.kafka.nodes"="localhost:9092",\n    "trino.kafka.table-description-supplier"="CONFLUENT",\n    "trino.kafka.confluent-schema-registry-url"="http://localhost:8081",\n    "trino.kafka.hide-internal-columns" = "false"\n);\n'))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"BigQuery"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'create catalog bigquery_catalog properties (\n    "type"="trino-connector",\n    "trino.connector.name"="bigquery",\n    "trino.bigquery.project-id"="steam-circlet-388406",\n    "trino.bigquery.credentials-file"="/path/to/application_default_credentials.json"\n);\n')))))}g.isMDXComponent=!0}}]);