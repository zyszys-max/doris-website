"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([[472530],{15680:(e,t,a)=>{a.d(t,{xA:()=>m,yg:()=>c});var n=a(296540);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},m=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},g=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),u=p(a),g=r,c=u["".concat(s,".").concat(g)]||u[g]||d[g]||o;return a?n.createElement(c,l(l({ref:t},m),{},{components:a})):n.createElement(c,l({ref:t},m))}));function c(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,l=new Array(o);l[0]=g;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[u]="string"==typeof e?e:r,l[1]=i;for(var p=2;p<o;p++)l[p]=a[p];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}g.displayName="MDXCreateElement"},698473:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var n=a(58168),r=(a(296540),a(15680));const o={title:"Routine Load",language:"en"},l=void 0,i={unversionedId:"data-operate/import/import-way/routine-load-manual",id:"version-1.2/data-operate/import/import-way/routine-load-manual",title:"Routine Load",description:"\x3c!--",source:"@site/versioned_docs/version-1.2/data-operate/import/import-way/routine-load-manual.md",sourceDirName:"data-operate/import/import-way",slug:"/data-operate/import/import-way/routine-load-manual",permalink:"/docs/1.2/data-operate/import/import-way/routine-load-manual",draft:!1,tags:[],version:"1.2",frontMatter:{title:"Routine Load",language:"en"},sidebar:"docs",previous:{title:"Broker Load",permalink:"/docs/1.2/data-operate/import/import-way/broker-load-manual"},next:{title:"Spark Load",permalink:"/docs/1.2/data-operate/import/import-way/spark-load-manual"}},s={},p=[{value:"Glossary",id:"glossary",level:2},{value:"Principle",id:"principle",level:2},{value:"Kafka Routine load",id:"kafka-routine-load",level:2},{value:"Usage restrictions",id:"usage-restrictions",level:3},{value:"Create a routine load task",id:"create-a-routine-load-task",level:3},{value:"Viewing Job Status",id:"viewing-job-status",level:3},{value:"Modify job properties",id:"modify-job-properties",level:3},{value:"Job Control",id:"job-control",level:3},{value:"Other notes",id:"other-notes",level:2},{value:"Related Parameters",id:"related-parameters",level:2},{value:"More help",id:"more-help",level:2}],m={toc:p},u="wrapper";function d(e){let{components:t,...a}=e;return(0,r.yg)(u,(0,n.A)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"routine-load"},"Routine Load"),(0,r.yg)("p",null,"The Routine Load feature provides users with a way to automatically load data from a specified data source."),(0,r.yg)("p",null,"This document describes the implementation principles, usage, and best practices of this feature."),(0,r.yg)("h2",{id:"glossary"},"Glossary"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"RoutineLoadJob: A routine load job submitted by the user."),(0,r.yg)("li",{parentName:"ul"},"JobScheduler: A routine load job scheduler for scheduling and dividing a RoutineLoadJob into multiple Tasks."),(0,r.yg)("li",{parentName:"ul"},"Task: RoutineLoadJob is divided by JobScheduler according to the rules."),(0,r.yg)("li",{parentName:"ul"},"TaskScheduler: Task Scheduler. Used to schedule the execution of a Task.")),(0,r.yg)("h2",{id:"principle"},"Principle"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"         +---------+\n         |  Client |\n         +----+----+\n              |\n+-----------------------------+\n| FE          |               |\n| +-----------v------------+  |\n| |                        |  |\n| |   Routine Load Job     |  |\n| |                        |  |\n| +---+--------+--------+--+  |\n|     |        |        |     |\n| +---v--+ +---v--+ +---v--+  |\n| | task | | task | | task |  |\n| +--+---+ +---+--+ +---+--+  |\n|    |         |        |     |\n+-----------------------------+\n     |         |        |\n     v         v        v\n +---+--+   +--+---+   ++-----+\n |  BE  |   |  BE  |   |  BE  |\n +------+   +------+   +------+\n\n")),(0,r.yg)("p",null,"As shown above, the client submits a routine load job to FE."),(0,r.yg)("p",null,"FE splits an load job into several Tasks via JobScheduler. Each Task is responsible for loading a specified portion of the data. The Task is assigned by the TaskScheduler to the specified BE."),(0,r.yg)("p",null,"On the BE, a Task is treated as a normal load task and loaded via the Stream Load load mechanism. After the load is complete, report to FE."),(0,r.yg)("p",null,"The JobScheduler in the FE continues to generate subsequent new Tasks based on the reported results, or retry the failed Task."),(0,r.yg)("p",null,"The entire routine load job completes the uninterrupted load of data by continuously generating new Tasks."),(0,r.yg)("h2",{id:"kafka-routine-load"},"Kafka Routine load"),(0,r.yg)("p",null,"Currently we only support routine load from the Kafka system. This section details Kafka's routine use and best practices."),(0,r.yg)("h3",{id:"usage-restrictions"},"Usage restrictions"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Support unauthenticated Kafka access and Kafka clusters certified by SSL."),(0,r.yg)("li",{parentName:"ol"},"The supported message format is csv text or json format. Each message is a line in csv format, and the end of the line does not contain a ** line break."),(0,r.yg)("li",{parentName:"ol"},"Kafka 0.10.0.0 (inclusive) or above is supported by default. If you want to use Kafka versions below 0.10.0.0 (0.9.0, 0.8.2, 0.8.1, 0.8.0), you need to modify the configuration of be, set the value of kafka_broker_version_fallback to be the older version, or directly set the value of property.broker.version.fallback to the old version when creating routine load. The cost of the old version is that some of the new features of routine load may not be available, such as setting the offset of the kafka partition by time.")),(0,r.yg)("h3",{id:"create-a-routine-load-task"},"Create a routine load task"),(0,r.yg)("p",null,"The detailed syntax for creating a routine import task can be found in ","[CREATE ROUTINE LOAD]","(... /... /... /sql-manual/sql-reference/Data-Manipulation-Statements/Load/CREATE-ROUTINE-LOAD.md) after connecting to Doris  command manual, or execute ",(0,r.yg)("inlineCode",{parentName:"p"},"HELP ROUTINE LOAD;")," for syntax help."),(0,r.yg)("p",null,"Here we illustrate how to create Routine Load tasks with a few examples."),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Create a Kafka example import task named test1 for example_tbl of example_db. Specify the column separator and group.id and client.id, and automatically consume all partitions by default and subscribe from the location where data is available (OFFSET_BEGINNING). ")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'CREATE ROUTINE LOAD example_db.test1 ON example_tbl\n        COLUMNS TERMINATED BY ",",\n        COLUMNS(k1, k2, k3, v1, v2, v3 = k1 * 100)\n        PROPERTIES\n        (\n            "desired_concurrent_number"="3",\n            "max_batch_interval" = "20",\n            "max_batch_rows" = "300000",\n            "max_batch_size" = "209715200",\n            "strict_mode" = "false"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n            "kafka_topic" = "my_topic",\n            "property.group.id" = "xxx",\n            "property.client.id" = "xxx",\n            "property.kafka_default_offsets" = "OFFSET_BEGINNING"\n        );\n')),(0,r.yg)("ol",{start:2},(0,r.yg)("li",{parentName:"ol"},"Create a Kafka example import task named test1 for example_tbl of example_db in ",(0,r.yg)("strong",{parentName:"li"},"strict mode"),".")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'CREATE ROUTINE LOAD example_db.test1 ON example_tbl\n        COLUMNS(k1, k2, k3, v1, v2, v3 = k1 * 100),\n        WHERE k1 > 100 and k2 like "%doris%"\n        PROPERTIES\n        (\n            "desired_concurrent_number"="3",\n            "max_batch_interval" = "20",\n            "max_batch_rows" = "300000",\n            "max_batch_size" = "209715200",\n            "strict_mode" = "true"\n        )\n        FROM KAFKA\n        (\n            "kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n            "kafka_topic" = "my_topic",\n            "kafka_partitions" = "0,1,2,3",\n            "kafka_offsets" = "101,0,0,200"\n        );\n')),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Notes\uff1a"),(0,r.yg)("p",{parentName:"blockquote"},'"strict_mode" = "true"')),(0,r.yg)("ol",{start:3},(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Example of importing data in Json format"),(0,r.yg)("p",{parentName:"li"},"Routine Load only supports the following two types of json formats"),(0,r.yg)("p",{parentName:"li"},"The first one has only one record and is a json object."))),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-json"},'{"category":"a9jadhx","author":"test","price":895}\n')),(0,r.yg)("p",null,"The second one is a json array, which can contain multiple records"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-json"},'[\n    {   \n        "category":"11",\n        "author":"4avc",\n        "price":895,\n        "timestamp":1589191587\n    },\n    {\n        "category":"22",\n        "author":"2avc",\n        "price":895,\n        "timestamp":1589191487\n    },\n    {\n        "category":"33",\n        "author":"3avc",\n        "price":342,\n        "timestamp":1589191387\n    }\n]\n')),(0,r.yg)("p",null,"Create the Doris data table to be imported"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'CREATE TABLE `example_tbl` (\n   `category` varchar(24) NULL COMMENT "",\n   `author` varchar(24) NULL COMMENT "",\n   `timestamp` bigint(20) NULL COMMENT "",\n   `dt` int(11) NULL COMMENT "",\n   `price` double REPLACE\n) ENGINE=OLAP\nAGGREGATE KEY(`category`,`author`,`timestamp`,`dt`)\nCOMMENT "OLAP"\nPARTITION BY RANGE(`dt`)\n(\n  PARTITION p0 VALUES [("-2147483648"), ("20200509")),\n    PARTITION p20200509 VALUES [("20200509"), ("20200510")),\n    PARTITION p20200510 VALUES [("20200510"), ("20200511")),\n    PARTITION p20200511 VALUES [("20200511"), ("20200512"))\n)\nDISTRIBUTED BY HASH(`category`,`author`,`timestamp`) BUCKETS 4\nPROPERTIES (\n    "replication_num" = "1"\n);\n')),(0,r.yg)("p",null,"Import json data in simple mode"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'CREATE ROUTINE LOAD example_db.test_json_label_1 ON table1\nCOLUMNS(category,price,author)\nPROPERTIES\n(\n    "desired_concurrent_number"="3",\n    "max_batch_interval" = "20",\n    "max_batch_rows" = "300000",\n    "max_batch_size" = "209715200",\n    "strict_mode" = "false",\n    "format" = "json"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n    "kafka_topic" = "my_topic",\n    "kafka_partitions" = "0,1,2",\n    "kafka_offsets" = "0,0,0"\n );\n')),(0,r.yg)("p",null,"Accurate import of data in json format"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'CREATE ROUTINE LOAD example_db.test1 ON example_tbl\nCOLUMNS(category, author, price, timestamp, dt=from_unixtime(timestamp, \'%Y%m%d\'))\nPROPERTIES\n(\n    "desired_concurrent_number"="3",\n    "max_batch_interval" = "20",\n    "max_batch_rows" = "300000",\n    "max_batch_size" = "209715200",\n    "strict_mode" = "false",\n    "format" = "json",\n    "jsonpaths" = "[\\"$.category\\",\\"$.author\\",\\"$.price\\",\\"$.timestamp\\"]",\n    "strip_outer_array" = "true"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n    "kafka_topic" = "my_topic",\n    "kafka_partitions" = "0,1,2",\n    "kafka_offsets" = "0,0,0"\n);\n')),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},(0,r.yg)("strong",{parentName:"p"},"Notes:")," "),(0,r.yg)("p",{parentName:"blockquote"},"The partition field ",(0,r.yg)("inlineCode",{parentName:"p"},"dt")," in the table is not in our data, but is converted in our Routine load statement by ",(0,r.yg)("inlineCode",{parentName:"p"},"dt=from_unixtime(timestamp, '%Y%m%d')"))),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"strict mode import relationship with source data")),(0,r.yg)("p",null,"Here is an example with a column type of TinyInt"),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Notes: When a column in the table allows importing null values")),(0,r.yg)("blockquote",null),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"source data"),(0,r.yg)("th",{parentName:"tr",align:null},"source data example"),(0,r.yg)("th",{parentName:"tr",align:null},"string to int"),(0,r.yg)("th",{parentName:"tr",align:null},"strict_mode"),(0,r.yg)("th",{parentName:"tr",align:null},"result"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Null value"),(0,r.yg)("td",{parentName:"tr",align:null},"\\N"),(0,r.yg)("td",{parentName:"tr",align:null},"N/A"),(0,r.yg)("td",{parentName:"tr",align:null},"true or false"),(0,r.yg)("td",{parentName:"tr",align:null},"NULL")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"not null"),(0,r.yg)("td",{parentName:"tr",align:null},"aaa or 2000"),(0,r.yg)("td",{parentName:"tr",align:null},"NULL"),(0,r.yg)("td",{parentName:"tr",align:null},"true"),(0,r.yg)("td",{parentName:"tr",align:null},"invalid data(filtered)")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"not null"),(0,r.yg)("td",{parentName:"tr",align:null},"aaa"),(0,r.yg)("td",{parentName:"tr",align:null},"NULL"),(0,r.yg)("td",{parentName:"tr",align:null},"false"),(0,r.yg)("td",{parentName:"tr",align:null},"NULL")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"not null"),(0,r.yg)("td",{parentName:"tr",align:null},"1"),(0,r.yg)("td",{parentName:"tr",align:null},"1"),(0,r.yg)("td",{parentName:"tr",align:null},"true or false"),(0,r.yg)("td",{parentName:"tr",align:null},"correct data")))),(0,r.yg)("p",null,"Here is an example with the column type Decimal(1,0)"),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Notes:"),(0,r.yg)("p",{parentName:"blockquote"}," When the columns in the table allow importing null values")),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"source data"),(0,r.yg)("th",{parentName:"tr",align:null},"source data example"),(0,r.yg)("th",{parentName:"tr",align:null},"string to int"),(0,r.yg)("th",{parentName:"tr",align:null},"strict_mode"),(0,r.yg)("th",{parentName:"tr",align:null},"result"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Null value"),(0,r.yg)("td",{parentName:"tr",align:null},"\\N"),(0,r.yg)("td",{parentName:"tr",align:null},"N/A"),(0,r.yg)("td",{parentName:"tr",align:null},"true or false"),(0,r.yg)("td",{parentName:"tr",align:null},"NULL")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"not null"),(0,r.yg)("td",{parentName:"tr",align:null},"aaa"),(0,r.yg)("td",{parentName:"tr",align:null},"NULL"),(0,r.yg)("td",{parentName:"tr",align:null},"true"),(0,r.yg)("td",{parentName:"tr",align:null},"invalid data(filtered)")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"not null"),(0,r.yg)("td",{parentName:"tr",align:null},"aaa"),(0,r.yg)("td",{parentName:"tr",align:null},"NULL"),(0,r.yg)("td",{parentName:"tr",align:null},"false"),(0,r.yg)("td",{parentName:"tr",align:null},"NULL")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"not null"),(0,r.yg)("td",{parentName:"tr",align:null},"1 or 10"),(0,r.yg)("td",{parentName:"tr",align:null},"1"),(0,r.yg)("td",{parentName:"tr",align:null},"true or false"),(0,r.yg)("td",{parentName:"tr",align:null},"correct data")))),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Notes:"),(0,r.yg)("p",{parentName:"blockquote"}," Although 10 is an out-of-range value, it is not affected by strict mode because its type meets the decimal requirement. 10 will eventually be filtered in other ETL processing processes. But it will not be filtered by strict mode.")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Accessing an SSL-certified Kafka cluster")),(0,r.yg)("p",null,"Accessing an SSL-certified Kafka cluster requires the user to provide the certificate file (ca.pem) used to authenticate the Kafka Broker's public key. If the Kafka cluster also has client authentication enabled, the client's public key (client.pem), the key file (client.key), and the key password are also required. The required files need to be uploaded to Doris first via the ",(0,r.yg)("inlineCode",{parentName:"p"},"CREAE FILE")," command, ",(0,r.yg)("strong",{parentName:"p"},"and the catalog name is ",(0,r.yg)("inlineCode",{parentName:"strong"},"kafka")),". See ",(0,r.yg)("inlineCode",{parentName:"p"},"HELP CREATE FILE;")," for help with the ",(0,r.yg)("inlineCode",{parentName:"p"},"CREATE FILE")," command. Here are some examples."),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"uploading a file")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'CREATE FILE "ca.pem" PROPERTIES("url" = "https://example_url/kafka-key/ca.pem", "catalog" = "kafka");\nCREATE FILE "client.key" PROPERTIES("url" = "https://example_urlkafka-key/client.key", "catalog" = "kafka");\nCREATE FILE "client.pem" PROPERTIES("url" = "https://example_url/kafka-key/client.pem", "catalog" = "kafka");\n')),(0,r.yg)("ol",{start:2},(0,r.yg)("li",{parentName:"ol"},"Create routine import jobs")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'CREATE ROUTINE LOAD db1.job1 on tbl1\nPROPERTIES\n(\n    "desired_concurrent_number"="1"\n)\nFROM KAFKA\n(\n    "kafka_broker_list"= "broker1:9091,broker2:9091",\n    "kafka_topic" = "my_topic",\n    "property.security.protocol" = "ssl",\n    "property.ssl.ca.location" = "FILE:ca.pem",\n    "property.ssl.certificate.location" = "FILE:client.pem",\n    "property.ssl.key.location" = "FILE:client.key",\n    "property.ssl.key.password" = "abcdefg"\n);\n')),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Doris accesses Kafka clusters through Kafka's C++ API ",(0,r.yg)("inlineCode",{parentName:"p"},"librdkafka"),". The parameters supported by ",(0,r.yg)("inlineCode",{parentName:"p"},"librdkafka")," can be found in"),(0,r.yg)("p",{parentName:"blockquote"},"[https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md]","(",(0,r.yg)("a",{parentName:"p",href:"https://github.com/edenhill/librdkafka/blob/master/"},"https://github.com/edenhill/librdkafka/blob/master/")," CONFIGURATION.md)")),(0,r.yg)("version",{since:"1.2"},(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Accessing a Kerberos-certified Kafka cluster")),(0,r.yg)("p",null,"Accessing a Kerberos-certified Kafka cluster. The following configurations need to be added:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"security.protocol=SASL_PLAINTEXT : Use SASL plaintext"),(0,r.yg)("li",{parentName:"ul"},"sasl.kerberos.service.name=$SERVICENAME : Broker service name"),(0,r.yg)("li",{parentName:"ul"},"sasl.kerberos.keytab=/etc/security/keytabs/${CLIENT_NAME}.keytab : Client keytab location"),(0,r.yg)("li",{parentName:"ul"},"sasl.kerberos.principal=${CLIENT_NAME}/${CLIENT_HOST} : sasl.kerberos.principal")),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Create routine import jobs"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'CREATE ROUTINE LOAD db1.job1 on tbl1\nPROPERTIES (\n"desired_concurrent_number"="1",\n )\nFROM KAFKA\n(\n    "kafka_broker_list" = "broker1:9092,broker2:9092",\n    "kafka_topic" = "my_topic",\n    "property.security.protocol" = "SASL_PLAINTEXT",\n    "property.sasl.kerberos.service.name" = "kafka",\n    "property.sasl.kerberos.keytab" = "/etc/krb5.keytab",\n    "property.sasl.kerberos.principal" = "doris@YOUR.COM"\n);\n')))),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Note:")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"To enable Doris to access the Kafka cluster with Kerberos authentication enabled, you need to deploy the Kerberos client kinit on all running nodes of the Doris cluster, configure krb5.conf, and fill in KDC service information."),(0,r.yg)("li",{parentName:"ul"},"Configure property.sasl.kerberos The value of keytab needs to specify the absolute path of the keytab local file and allow Doris processes to access the local file."))),(0,r.yg)("h3",{id:"viewing-job-status"},"Viewing Job Status"),(0,r.yg)("p",null,"Specific commands and examples to view the status of ",(0,r.yg)("strong",{parentName:"p"},"jobs")," can be viewed with the ",(0,r.yg)("inlineCode",{parentName:"p"},"HELP SHOW ROUTINE LOAD;")," command."),(0,r.yg)("p",null,"Specific commands and examples to view the status of ",(0,r.yg)("strong",{parentName:"p"},"tasks")," running can be viewed with the ",(0,r.yg)("inlineCode",{parentName:"p"},"HELP SHOW ROUTINE LOAD TASK;")," command."),(0,r.yg)("p",null,"Only currently running tasks can be viewed; closed and unstarted tasks cannot be viewed."),(0,r.yg)("h3",{id:"modify-job-properties"},"Modify job properties"),(0,r.yg)("p",null,"Users can modify jobs that have already been created. The details can be viewed with the ",(0,r.yg)("inlineCode",{parentName:"p"},"HELP ALTER ROUTINE LOAD;")," command or see ","[ALTER ROUTINE LOAD]","(... /... /... /sql-manual/sql-reference/Data-Manipulation-Statements/Load/ALTER-ROUTINE-LOAD.md)."),(0,r.yg)("h3",{id:"job-control"},"Job Control"),(0,r.yg)("p",null,"The user can control the stop, pause and restart of jobs with the ",(0,r.yg)("inlineCode",{parentName:"p"},"STOP/PAUSE/RESUME")," commands. Help and examples can be viewed with the ",(0,r.yg)("inlineCode",{parentName:"p"},"HELP STOP ROUTINE LOAD;")," ",(0,r.yg)("inlineCode",{parentName:"p"},"HELP PAUSE ROUTINE LOAD;")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"HELP RESUME ROUTINE LOAD;")," commands."),(0,r.yg)("h2",{id:"other-notes"},"Other notes"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"The relationship between a routine import job and an ALTER TABLE operation"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Example import does not block SCHEMA CHANGE and ROLLUP operations. However, note that if the column mapping relationships do not match after the SCHEMA CHANGE completes, it can cause a spike in error data for the job and eventually cause the job to pause. It is recommended that you reduce this problem by explicitly specifying column mapping relationships in routine import jobs and by adding Nullable columns or columns with Default values."),(0,r.yg)("li",{parentName:"ul"},"Deleting a Partition of a table may cause the imported data to fail to find the corresponding Partition and the job to enter a pause. 2."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Relationship between routine import jobs and other import jobs (LOAD, DELETE, INSERT)"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"There is no conflict between the routine import and other LOAD operations and INSERT operations."),(0,r.yg)("li",{parentName:"ul"},"When the DELETE operation is executed, the corresponding table partition cannot have any ongoing import jobs. Therefore, before executing DELETE operation, you may need to suspend the routine import job and wait until all the issued tasks are completed before executing DELETE. 3."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"The relationship between routine import and DROP DATABASE/TABLE operations"),(0,r.yg)("p",{parentName:"li"},"When the database or table corresponding to the routine import is deleted, the job will automatically CANCEL.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"The relationship between kafka type routine import jobs and kafka topic"),(0,r.yg)("p",{parentName:"li"},"When the ",(0,r.yg)("inlineCode",{parentName:"p"},"kafka_topic")," declared by the user in the create routine import statement does not exist in the kafka cluster."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"If the broker of the user's kafka cluster has ",(0,r.yg)("inlineCode",{parentName:"li"},"auto.create.topics.enable = true")," set, then ",(0,r.yg)("inlineCode",{parentName:"li"},"kafka_topic")," will be created automatically first, and the number of partitions created automatically is determined by the configuration of the broker in the ",(0,r.yg)("strong",{parentName:"li"},"user's kafka cluster")," with ",(0,r.yg)("inlineCode",{parentName:"li"},"num. partitions"),". The routine job will keep reading data from the topic as normal."),(0,r.yg)("li",{parentName:"ul"},"If the broker in the user's kafka cluster has ",(0,r.yg)("inlineCode",{parentName:"li"},"auto.create.topics.enable = false")," set, the topic will not be created automatically and the routine job will be suspended with a status of ",(0,r.yg)("inlineCode",{parentName:"li"},"PAUSED")," before any data is read.")),(0,r.yg)("p",{parentName:"li"},"So, if you want the kafka topic to be automatically created by the routine when it does not exist, just set ",(0,r.yg)("inlineCode",{parentName:"p"},"auto.create.topics.enable = true")," for the broker in the ",(0,r.yg)("strong",{parentName:"p"},"user's kafka cluster"),".")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Problems that may arise in network isolated environments In some environments there are isolation measures for network segments and domain name resolution, so care needs to be taken"),(0,r.yg)("ol",{parentName:"li"},(0,r.yg)("li",{parentName:"ol"},"the Broker list specified in the Create Routine load task must be accessible by the Doris service"),(0,r.yg)("li",{parentName:"ol"},"If ",(0,r.yg)("inlineCode",{parentName:"li"},"advertised.listeners")," is configured in Kafka, the addresses in ",(0,r.yg)("inlineCode",{parentName:"li"},"advertised.listeners")," must be accessible to the Doris service"))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Specify the Partition and Offset for consumption"),(0,r.yg)("p",{parentName:"li"},"Doris supports specifying a Partition and Offset to start consumption. The new version also supports the ability to specify time points for consumption. The configuration of the corresponding parameters is explained here."),(0,r.yg)("p",{parentName:"li"},"There are three relevant parameters."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"kafka_partitions"),': Specify the list of partitions to be consumed, e.g., "0, 1, 2, 3".'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"kafka_offsets"),": specifies the starting offset of each partition, which must correspond to the number of ",(0,r.yg)("inlineCode",{parentName:"li"},"kafka_partitions"),' list. For example: "1000, 1000, 2000, 2000"'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"property.kafka_default_offset"),": specifies the default starting offset of the partitions.")),(0,r.yg)("p",{parentName:"li"},"When creating an import job, these three parameters can have the following combinations."),(0,r.yg)("table",{parentName:"li"},(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"combinations"),(0,r.yg)("th",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"th"},"kafka_partitions")),(0,r.yg)("th",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"th"},"kafka_offsets")),(0,r.yg)("th",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"th"},"property.kafka_default_offset")),(0,r.yg)("th",{parentName:"tr",align:null},"behavior"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"1"),(0,r.yg)("td",{parentName:"tr",align:null},"No"),(0,r.yg)("td",{parentName:"tr",align:null},"No"),(0,r.yg)("td",{parentName:"tr",align:null},"No"),(0,r.yg)("td",{parentName:"tr",align:null},"The system will automatically find all partitions corresponding to the topic and start consuming them from OFFSET_END")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"2"),(0,r.yg)("td",{parentName:"tr",align:null},"No"),(0,r.yg)("td",{parentName:"tr",align:null},"No"),(0,r.yg)("td",{parentName:"tr",align:null},"Yes"),(0,r.yg)("td",{parentName:"tr",align:null},"The system will automatically find all the partitions corresponding to the topic and start consuming them from the default offset location.")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"3"),(0,r.yg)("td",{parentName:"tr",align:null},"Yes"),(0,r.yg)("td",{parentName:"tr",align:null},"No"),(0,r.yg)("td",{parentName:"tr",align:null},"No"),(0,r.yg)("td",{parentName:"tr",align:null},"The system will start consuming from the OFFSET_END of the specified partition.")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"4"),(0,r.yg)("td",{parentName:"tr",align:null},"Yes"),(0,r.yg)("td",{parentName:"tr",align:null},"Yes"),(0,r.yg)("td",{parentName:"tr",align:null},"No"),(0,r.yg)("td",{parentName:"tr",align:null},"The system will start consuming at the specified offset of the specified partition.")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"5"),(0,r.yg)("td",{parentName:"tr",align:null},"Yes"),(0,r.yg)("td",{parentName:"tr",align:null},"No"),(0,r.yg)("td",{parentName:"tr",align:null},"Yes"),(0,r.yg)("td",{parentName:"tr",align:null},"The system will start consuming at the default offset of the specified partition"))))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"The difference between STOP and PAUSE"),(0,r.yg)("p",{parentName:"li"},"FE will automatically clean up the ROUTINE LOAD in STOP status periodically, while the PAUSE status can be restored to enable again."))),(0,r.yg)("h2",{id:"related-parameters"},"Related Parameters"),(0,r.yg)("p",null,"Some system configuration parameters can affect the use of routine import."),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"max_routine_load_task_concurrent_num"),(0,r.yg)("p",{parentName:"li"},"FE configuration item, defaults to 5 and can be modified at runtime. This parameter limits the maximum number of concurrent subtasks for a routine import job. It is recommended to keep the default value. Setting it too large may result in too many concurrent tasks and consume cluster resources.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"max_routine_load_task_num_per_be"),(0,r.yg)("p",{parentName:"li"},"FE configuration item, default is 5, can be modified at runtime. This parameter limits the maximum number of concurrently executed subtasks per BE node. It is recommended to keep the default value. If set too large, it may lead to too many concurrent tasks and consume cluster resources.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"max_routine_load_job_num"),(0,r.yg)("p",{parentName:"li"},"FE configuration item, default is 100, can be modified at runtime. This parameter limits the total number of routine import jobs, including the states NEED_SCHEDULED, RUNNING, PAUSE. After this, no new jobs can be submitted.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"max_consumer_num_per_group"),(0,r.yg)("p",{parentName:"li"},"BE configuration item, default is 3. This parameter indicates the maximum number of consumers that can be generated for data consumption in a subtask. For a Kafka data source, a consumer may consume one or more kafka partitions. If there are only 2 partitions, only 2 consumers are generated, each consuming 1 partition. 5. push_write_mby")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"max_tolerable_backend_down_num "),(0,r.yg)("p",{parentName:"li"},"FE configuration item, the default value is 0. Doris can PAUSED job rescheduling to RUNNING if certain conditions are met. 0 means rescheduling is allowed only if all BE nodes are ALIVE.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"period_of_auto_resume_min "),(0,r.yg)("p",{parentName:"li"},"FE configuration item, the default is 5 minutes, Doris rescheduling will only be attempted up to 3 times within the 5 minute period. If all 3 attempts fail, the current task is locked and no further scheduling is performed. However, manual recovery can be done through human intervention."))),(0,r.yg)("h2",{id:"more-help"},"More help"),(0,r.yg)("p",null,"For more detailed syntax on the use of ",(0,r.yg)("strong",{parentName:"p"},"Routine Load"),", you can type ",(0,r.yg)("inlineCode",{parentName:"p"},"HELP ROUTINE LOAD")," at the Mysql client command line for more help."))}d.isMDXComponent=!0}}]);