"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([[245739],{15680:(e,t,n)=>{n.d(t,{xA:()=>u,yg:()=>g});var a=n(296540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=c(n),m=r,g=p["".concat(l,".").concat(m)]||p[m]||d[m]||i;return n?a.createElement(g,o(o({ref:t},u),{},{components:n})):a.createElement(g,o({ref:t},u))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:r,o[1]=s;for(var c=2;c<i;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},379195:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var a=n(58168),r=(n(296540),n(15680));const i={title:"Regression Testing",language:"en"},o=void 0,s={unversionedId:"developer-guide/regression-testing",id:"developer-guide/regression-testing",title:"Regression Testing",description:"\x3c!--",source:"@site/community/developer-guide/regression-testing.md",sourceDirName:"developer-guide",slug:"/developer-guide/regression-testing",permalink:"/community/developer-guide/regression-testing",draft:!1,tags:[],version:"current",frontMatter:{title:"Regression Testing",language:"en"},sidebar:"community",previous:{title:"Github Checks",permalink:"/community/developer-guide/github-checks"},next:{title:"Doris Team",permalink:"/community/team"}},l={},c=[{value:"Basic Concepts",id:"basic-concepts",level:2},{value:"Steps",id:"steps",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Default Configurations of the Framework",id:"default-configurations-of-the-framework",level:2},{value:"Steps to Write Test Cases",id:"steps-to-write-test-cases",level:2},{value:"Action",id:"action",level:2},{value:"sql action",id:"sql-action",level:3},{value:"qt action",id:"qt-action",level:3},{value:"test action",id:"test-action",level:3},{value:"explain action",id:"explain-action",level:3},{value:"streamLoad action",id:"streamload-action",level:3},{value:"Other Actions",id:"other-actions",level:3},{value:"Startup Script Examples",id:"startup-script-examples",level:2},{value:"Generate <code>.out</code> File Based on the Query Result",id:"generate-out-file-based-on-the-query-result",level:2},{value:"Suite Plug-in",id:"suite-plug-in",level:2},{value:"CI/CD Support",id:"cicd-support",level:2},{value:"TeamCity",id:"teamcity",level:3},{value:"E2E Test with External Data Sources",id:"e2e-test-with-external-data-sources",level:2}],u={toc:c},p="wrapper";function d(e){let{components:t,...n}=e;return(0,r.yg)(p,(0,a.A)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"regression-testing"},"Regression Testing"),(0,r.yg)("h2",{id:"basic-concepts"},"Basic Concepts"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("inlineCode",{parentName:"li"},"Suite"),": A test case (the file name)"),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("inlineCode",{parentName:"li"},"Group"),": A test set (the directory that the test case belongs to)"),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("inlineCode",{parentName:"li"},"Action"),": An encapsulated test action, such as ",(0,r.yg)("inlineCode",{parentName:"li"},"sql_action"),"  for SQL execution, ",(0,r.yg)("inlineCode",{parentName:"li"},"test_action"),"  for result verification, and ",(0,r.yg)("inlineCode",{parentName:"li"},"streamLoad_action"),"  for data ingestion.")),(0,r.yg)("h2",{id:"steps"},"Steps"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Get the clusters ready"),(0,r.yg)("li",{parentName:"ol"},"Modify the configuration files ",(0,r.yg)("inlineCode",{parentName:"li"},"${DORIS_HOME}/regression-test/conf/regression-conf.groovy"),", set items, such as JDBC URL and user"),(0,r.yg)("li",{parentName:"ol"},"Create the test case files and write the test cases"),(0,r.yg)("li",{parentName:"ol"},"If a test case includes a ",(0,r.yg)("inlineCode",{parentName:"li"},"qt")," Action, you also need to create the relevant data files. For example, the case ",(0,r.yg)("inlineCode",{parentName:"li"},"suites/demo/qt_action.groovy")," will require a TSV file ",(0,r.yg)("inlineCode",{parentName:"li"},"data/demo/qt_action.out")," for output verification."),(0,r.yg)("li",{parentName:"ol"},"Run ",(0,r.yg)("inlineCode",{parentName:"li"},"${DORIS_HOME}/run-regression-test.sh")," to test all cases, or run ",(0,r.yg)("inlineCode",{parentName:"li"},"${DORIS_HOME}/run-regression-test.sh --run <suiteName>"),'  to test a few cases. For more examples, please refer to the "Startup Script Examples" section on this page.')),(0,r.yg)("h2",{id:"directory-structure"},"Directory Structure"),(0,r.yg)("p",null,"Key files and directories to pay attention to:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("inlineCode",{parentName:"li"},"run-regression-test.sh"),": Startup script"),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("inlineCode",{parentName:"li"},"regression-conf.groovy"),": Default configurations for regression test"),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("inlineCode",{parentName:"li"},"data"),": where the the input and output data is"),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("inlineCode",{parentName:"li"},"suites"),": where test cases are")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"./${DORIS_HOME}\n    |-- **run-regression-test.sh**           Startup script for regression test\n    |-- regression-test\n    |   |-- plugins                          Plug-ins\n    |   |-- conf\n    |   |   |-- logback.xml                  Log configurations\n    |   |   |-- **regression-conf.groovy**   Default configurations\n    |   |\n    |   |-- framework                        framework source code for regression test\n    |   |-- **data**                         Input and output data files of test cases\n    |   |   |-- demo                         Input and output data files of demo\n    |   |   |-- correctness                  Input and output data files of correctness test cases\n    |   |   |-- performance                  Input and output data files of performance test cases\n    |   |   |-- utils                        Input and output data files of other utilities\n    |   |\n    |   |-- **suites**                       Test cases for regression testing\n    |       |-- demo                         Demo of test cases\n    |       |-- correctness                  Test cases of correctness tests\n    |       |-- performance                  Test cases of performance tests\n    |       |-- utils                        Other utilities\n    |\n    |-- output\n        |-- regression-test\n            |-- log                          Logs of regression testing\n")),(0,r.yg)("h2",{id:"default-configurations-of-the-framework"},"Default Configurations of the Framework"),(0,r.yg)("p",null,"Modify the JDBC and FE configurations based on the case."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-groovy"},'/* ============ Key parts to pay attention to ============ */\n// The default database, which will be created if the user does not create a database\ndefaultDb = "regression_test"\n\n// JDBC configurations\njdbcUrl = "jdbc:mysql://127.0.0.1:9030/?"\njdbcUser = "root"\njdbcPassword = ""\n\n// FE address configurations, used for Stream Load\nfeHttpAddress = "127.0.0.1:8030"\nfeHttpUser = "root"\nfeHttpPassword = ""\n\n/* ============ Configurations that usually do not require modifications ============ */\n\n// DORIS_HOME is loaded by run-regression-test.sh\n// which is java -DDORIS_HOME=./\n\n// Set the path for the test cases\nsuitePath = "${DORIS_HOME}/regression-test/suites"\n// Set the path for the input and output data\ndataPath = "${DORIS_HOME}/regression-test/data"\n// Set the path for the plug-ins\npluginPath = "${DORIS_HOME}/regression-test/plugins"\n\n// By default, all groups will be read. The groups are separated by comma. For example, "demo,performance".\n// This is dynamically specified and overwritten via run-regression-test.sh --run -g\ntestGroups = ""\n// By default, all suites will be read. This is dynamically specified and overwritten via run-regression-test.sh --run -s\ntestSuites = ""\n// Directories specified in this parameter will be loaded by default. It can be dynamically specified and overwritten via run-regression-test.sh --run -d\ntestDirectories = ""\n\n// Groups specified in this parameter will be excluded. It can be dynamically specified and overwritten via run-regression-test.sh --run -xg\nexcludeGroups = ""\n// Suites specified in this parameter will be excluded. It can be dynamically specified and overwritten via run-regression-test.sh --run -xs\nexcludeSuites = ""\n// Directories specified in this parameter will be excluded. It can be dynamically specified and overwritten via run-regression-test.sh --run -xd\nexcludeDirectories = ""\n\n// Other self-defined configurations\ncustomConf1 = "test_custom_conf_value"\n')),(0,r.yg)("h2",{id:"steps-to-write-test-cases"},"Steps to Write Test Cases"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Enter the ",(0,r.yg)("inlineCode",{parentName:"li"},"${DORIS_HOME}/regression-test")," directory"),(0,r.yg)("li",{parentName:"ol"},"Choose a directory based on the test. For correctness test, place the test in ",(0,r.yg)("inlineCode",{parentName:"li"},"suites/correctness"),"; for performance test, put the test case in ",(0,r.yg)("inlineCode",{parentName:"li"},"suites/performance"),"."),(0,r.yg)("li",{parentName:"ol"},"Create a groovy test case file, and add a few ",(0,r.yg)("inlineCode",{parentName:"li"},"Action")," for testing. ")),(0,r.yg)("h2",{id:"action"},"Action"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"Action")," refers to a test action, which is defined by DSL and provided in the test framework."),(0,r.yg)("h3",{id:"sql-action"},"sql action"),(0,r.yg)("p",null,"A ",(0,r.yg)("inlineCode",{parentName:"p"},"sql_action")," is used to commit SQL and obtain results. If the query fails, an error will be thrown."),(0,r.yg)("p",null,"Parameters:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"String sql: the input SQL string"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"return List<List<Object>>"),": the query result. If it is DDL/DML, a result of one row and one column will be returned and the value will be updateRowCount.")),(0,r.yg)("p",null,"You can find the following sample code in ",(0,r.yg)("inlineCode",{parentName:"p"},"${DORIS_HOME}/regression-test/suites/demo/sql_action.groovy"),": "),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-groovy"},'suite("sql_action", "demo") {\n    // execute sql and ignore result\n    sql "show databases"\n\n    // execute sql and get result, outer List denote rows, inner List denote columns in a single row\n    List<List<Object>> tables = sql "show tables"\n\n    // assertXxx() will invoke junit5\'s Assertions.assertXxx() dynamically\n    assertTrue(tables.size() >= 0) // test rowCount >= 0\n\n    // syntax error\n    try {\n        sql "a b c d e"\n        throw new IllegalStateException("Should be syntax error")\n    } catch (java.sql.SQLException t) {\n        assertTrue(true)\n    }\n\n    def testTable = "test_sql_action1"\n\n    try {\n        sql "DROP TABLE IF EXISTS ${testTable}"\n\n        // multi-line sql\n        def result1 = sql """\n                        CREATE TABLE IF NOT EXISTS ${testTable} (\n                            id int\n                        )\n                        DISTRIBUTED BY HASH(id) BUCKETS 1\n                        PROPERTIES (\n                          "replication_num" = "1"\n                        ) \n                        """\n\n        // DDL/DML return 1 row and 1 column, the only value is update row count\n        assertTrue(result1.size() == 1)\n        assertTrue(result1[0].size() == 1)\n        assertTrue(result1[0][0] == 0, "Create table should update 0 rows")\n\n        def result2 = sql "INSERT INTO test_sql_action1 values(1), (2), (3)"\n        assertTrue(result2.size() == 1)\n        assertTrue(result2[0].size() == 1)\n        assertTrue(result2[0][0] == 3, "Insert should update 3 rows")\n    } finally {\n        /**\n         * try_xxx(args) means:\n         *\n         * try {\n         *    return xxx(args)\n         * } catch (Throwable t) {\n         *     // do nothing\n         *     return null\n         * }\n         */\n        try_sql("DROP TABLE IF EXISTS ${testTable}")\n\n        // you can see the error sql will not throw exception and return\n        try {\n            def errorSqlResult = try_sql("a b c d e f g")\n            assertTrue(errorSqlResult == null)\n        } catch (Throwable t) {\n            assertTrue(false, "Never catch exception")\n        }\n    }\n\n    // order_sql(sqlStr) equals to sql(sqlStr, isOrder=true)\n    // sort result by string dict\n    def list = order_sql """\n                select 2\n                union all\n                select 1\n                union all\n                select null\n                union all\n                select 15\n                union all\n                select 3\n                """\n\n    assertEquals(null, list[0][0])\n    assertEquals(1, list[1][0])\n    assertEquals(15, list[2][0])\n    assertEquals(2, list[3][0])\n    assertEquals(3, list[4][0])\n}\n')),(0,r.yg)("h3",{id:"qt-action"},"qt action"),(0,r.yg)("p",null,"A ",(0,r.yg)("inlineCode",{parentName:"p"},"qt_action")," is used to commit SQL and verify the output results based on the ",(0,r.yg)("inlineCode",{parentName:"p"},".out")," TSV file."),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"String sql: the input SQL string"),(0,r.yg)("li",{parentName:"ul"},"return void")),(0,r.yg)("p",null,"You can find the following sample code in ",(0,r.yg)("inlineCode",{parentName:"p"},"${DORIS_HOME}/regression-test/suites/demo/qt_action.groovy"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-groovy"},'suite("qt_action", "demo") {\n    /**\n     * qt_xxx sql equals to quickTest(xxx, sql) in which xxx is tag.\n     * The result will be compared to the relevant file: ${DORIS_HOME}/regression_test/data/qt_action.out.\n     *\n     * If you want to generate .out tsv file, you can run -genOut or -forceGenOut.\n     * e.g\n     *   ${DORIS_HOME}/run-regression-test.sh --run qt_action -genOut\n     *   ${DORIS_HOME}/run-regression-test.sh --run qt_action -forceGenOut\n     */\n    qt_select "select 1, \'beijing\' union all select 2, \'shanghai\'"\n\n    qt_select2 "select 2"\n\n    // Order result by string dict then compare to .out file.\n    // order_qt_xxx sql equals to quickTest(xxx, sql, true).\n    order_qt_union_all  """\n                select 2\n                union all\n                select 1\n                union all\n                select null\n                union all\n                select 15\n                union all\n                select 3\n                """\n}\n')),(0,r.yg)("h3",{id:"test-action"},"test action"),(0,r.yg)("p",null,"A ",(0,r.yg)("inlineCode",{parentName:"p"},"test_action")," is to test with more complicated verification rules, such as to verify the number of rows, execution time, and the existence of errors"),(0,r.yg)("p",null,"Parameters:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"String sql: the input SQL string"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"List<List<Object>> result"),": provide a list to check if the output is equal to that list"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Iterator<Object> resultIterator"),": provide an iterator to check if the output is equal to that iterator"),(0,r.yg)("li",{parentName:"ul"},"String resultFile: provide a file URI (a relative path of a local file or a http(s) path) to check if the output is equal to the http response stream. The format is similar to that of the ",(0,r.yg)("inlineCode",{parentName:"li"},".out")," file, but there is no header or comment."),(0,r.yg)("li",{parentName:"ul"},"String exception: check if the error thrown includes certain strings"),(0,r.yg)("li",{parentName:"ul"},"long rowNum: check the number of rows"),(0,r.yg)("li",{parentName:"ul"},"long time: check if the execution time is shorter than this value, which is measured in millisecond"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Closure<List<List<Object>>, Throwable, Long, Long> check"),": self-defined callback validation function, in which you can input the query result, error thrown, and response time. When there is a callback validation function, other verification methods will fail.")),(0,r.yg)("p",null,"You can find the following sample code in ",(0,r.yg)("inlineCode",{parentName:"p"},"${DORIS_HOME}/regression-test/suites/demo/test_action.groovy"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-groovy"},'suite("test_action", "demo") {\n    test {\n        sql "abcdefg"\n        // check exception message contains\n        exception "errCode = 2, detailMessage = Syntax error"\n    }\n\n    test {\n        sql """\n            select *\n            from (\n                select 1 id\n                union all\n                select 2\n            ) a\n            order by id"""\n\n        // multi check condition\n\n        // check return 2 rows\n        rowNum 2\n        // execute time must <= 5000 millisecond\n        time 5000\n        // check result, must be 2 rows and 1 column, the first row is 1, second is 2\n        result(\n            [[1], [2]]\n        )\n    }\n\n    test {\n        sql "a b c d e f g"\n\n        // other check will not work because already declared a check callback\n        exception "aaaaaaaaa"\n\n        // callback\n        check { result, exception, startTime, endTime ->\n            // assertXxx() will invoke junit5\'s Assertions.assertXxx() dynamically\n            assertTrue(exception != null)\n        }\n    }\n\n    test {\n        sql  """\n                select 2\n                union all\n                select 1\n                union all\n                select null\n                union all\n                select 15\n                union all\n                select 3\n                """\n\n        check { result, ex, startTime, endTime ->\n            // same as order_sql(sqlStr)\n            result = sortRows(result)\n\n            assertEquals(null, result[0][0])\n            assertEquals(1, result[1][0])\n            assertEquals(15, result[2][0])\n            assertEquals(2, result[3][0])\n            assertEquals(3, result[4][0])\n        }\n    }\n\n    // execute sql and order query result, then compare to iterator\n    def selectValues = [1, 2, 3, 4]\n    test {\n        order true\n        sql selectUnionAll(selectValues)\n        resultIterator(selectValues.iterator())\n    }\n\n    // compare to data/demo/test_action.csv\n    test {\n        order true\n        sql selectUnionAll(selectValues)\n\n        // you can set to http://xxx or https://xxx\n        // and compare to http response body\n        resultFile "test_action.csv"\n    }\n}\n')),(0,r.yg)("h3",{id:"explain-action"},"explain action"),(0,r.yg)("p",null,"An ",(0,r.yg)("inlineCode",{parentName:"p"},"explain_action")," is used to check if the returned string contains certain strings."),(0,r.yg)("p",null,"Parameters:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"String sql: the SQL that needs to be explained"),(0,r.yg)("li",{parentName:"ul"},"String contains: check if certain strings are included. You can check for multiple strings at a time."),(0,r.yg)("li",{parentName:"ul"},"String notContains: check if certain strings are not included. You can check for multiple strings at a time."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Closure<String> check"),": self-defined callback validation function, by which you can obtain the returned string. When there is a callback validation function, other verification methods will fail."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Closure<String, Throwable, Long, Long> check"),": self-defined callback validation function, by which you can obtain the error thrown and response time")),(0,r.yg)("p",null,"You can find the following sample code in ",(0,r.yg)("inlineCode",{parentName:"p"},"${DORIS_HOME}/regression-test/suites/demo/explain_action.groovy"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-groovy"},'suite("explain_action", "demo") {\n    explain {\n        sql("select 100")\n\n        // contains("OUTPUT EXPRS:<slot 0> 100\\n") && contains("PARTITION: UNPARTITIONED\\n")\n        contains "OUTPUT EXPRS:<slot 0> 100\\n"\n        contains "PARTITION: UNPARTITIONED\\n"\n    }\n\n    explain {\n        sql("select 100")\n\n        // contains(" 100\\n") && !contains("abcdefg") && !("1234567")\n        contains " 100\\n"\n        notContains "abcdefg"\n        notContains "1234567"\n    }\n\n    explain {\n        sql("select 100")\n        // simple callback\n        check { explainStr -> explainStr.contains("abcdefg") || explainStr.contains(" 100\\n") }\n    }\n\n    explain {\n        sql("a b c d e")\n        // callback with exception and time\n        check { explainStr, exception, startTime, endTime ->\n            // assertXxx() will invoke junit5\'s Assertions.assertXxx() dynamically\n            assertTrue(exception != null)\n        }\n    }\n}\n')),(0,r.yg)("h3",{id:"streamload-action"},"streamLoad action"),(0,r.yg)("p",null,"A ",(0,r.yg)("inlineCode",{parentName:"p"},"streamLoad_action")," is used to ingest data."),(0,r.yg)("p",null,"Parameters:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"String db: database, set to the defaultDb in regression-conf.groovy"),(0,r.yg)("li",{parentName:"ul"},"String table: table name"),(0,r.yg)("li",{parentName:"ul"},"String file: the path of the file to be loaded. It can be a relative path under the data directory, or a HTTP URL."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Iterator<List<Object>> inputIterator"),": the iterator to be loaded"),(0,r.yg)("li",{parentName:"ul"},"String inputText: the text to be loaded (rarely used)"),(0,r.yg)("li",{parentName:"ul"},"InputStream inputStream: the stream to be loaded (rarely used)"),(0,r.yg)("li",{parentName:"ul"},"long time: check if the exection time is shorter than this value, which is measure in millisecond"),(0,r.yg)("li",{parentName:"ul"},"void set(String key, String value): set the header of the HTTP request for Stream Load, such as label and columnSeparator."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Closure<String, Throwable, Long, Long> check"),": self-defined callback validation function, by which you can obtain the returned result, error thrown, and response time. When there is a callback validation function, other verification methods will fail.")),(0,r.yg)("p",null,"You can find the following sample code in ",(0,r.yg)("inlineCode",{parentName:"p"},"${DORIS_HOME}/regression-test/suites/demo/streamLoad_action.groovy"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-groovy"},'suite("streamLoad_action", "demo") {\n\n    def tableName = "test_streamload_action1"\n\n    sql """\n            CREATE TABLE IF NOT EXISTS ${tableName} (\n                id int,\n                name varchar(255)\n            )\n            DISTRIBUTED BY HASH(id) BUCKETS 1\n            PROPERTIES (\n              "replication_num" = "1"\n            ) \n        """\n\n    streamLoad {\n        // you can skip declare db, because a default db already specify in ${DORIS_HOME}/conf/regression-conf.groovy\n        // db \'regression_test\'\n        table tableName\n\n        // default label is UUID:\n        // set \'label\' UUID.randomUUID().toString()\n\n        // default column_separator is specify in doris fe config, usually is \'\\t\'.\n        // this line change to \',\'\n        set \'column_separator\', \',\'\n\n        // relate to ${DORIS_HOME}/regression-test/data/demo/streamload_input.csv.\n        // also, you can stream load a http stream, e.g. http://xxx/some.csv\n        file \'streamload_input.csv\'\n\n        time 10000 // limit inflight 10s\n\n        // stream load action will check result, include Success status, and NumberTotalRows == NumberLoadedRows\n    }\n\n\n    // stream load 100 rows\n    def rowCount = 100\n    // range: [0, rowCount)\n    // or rangeClosed: [0, rowCount]\n    def rowIt = range(0, rowCount)\n            .mapToObj({i -> [i, "a_" + i]}) // change Long to List<Long, String>\n            .iterator()\n\n    streamLoad {\n        table tableName\n        // also, you can upload a memory iterator\n        inputIterator rowIt\n\n        // if declared a check callback, the default check condition will ignore.\n        // So you must check all condition\n        check { result, exception, startTime, endTime ->\n            if (exception != null) {\n                throw exception\n            }\n            log.info("Stream load result: ${result}".toString())\n            def json = parseJson(result)\n            assertEquals("success", json.Status.toLowerCase())\n            assertEquals(json.NumberTotalRows, json.NumberLoadedRows)\n            assertTrue(json.NumberLoadedRows > 0 && json.LoadBytes > 0)\n        }\n    }\n}\n')),(0,r.yg)("h3",{id:"other-actions"},"Other Actions"),(0,r.yg)("p",null,"Other actions include thread, lazyCheck, events, connect, and selectUnionAll. You can find examples for them in ",(0,r.yg)("inlineCode",{parentName:"p"},"${DORIS_HOME}/regression-test/suites/demo"),"."),(0,r.yg)("h2",{id:"startup-script-examples"},"Startup Script Examples"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},'# View parameter descriptions for scripts\n./run-regression-test.sh h\n\n# View parameter descriptions for framework\n./run-regression-test.sh --run -h\n\n# Test all cases\n./run-regression-test.sh \n\n# Delete test framework compilation results and test logs\n./run-regression-test.sh --clean\n\n# Test the suite named "sql_action". Currently, the test case file suffix is the same as the suiteName, so the corresponding test case file in this case is sql_action.groovy.\n./run-regression-test.sh --run sql_action \n\n# Test the suite whose name contains \'sql\'. **Remember to use single quotation marks.**\n./run-regression-test.sh --run \'*sql*\' \n\n# Test the "demo" and "perfomance" group\n./run-regression-test.sh --run -g \'demo,performance\'\n\n# Test the suite "sql_action" in the "demo" group\n./run-regression-test.sh --run -g demo -s sql_action\n\n# Test the suite "sql_action" in the demo directory\n./run-regression-test.sh --run -d demo -s sql_action\n\n# Test the suites under the demo directory except the "sql_action" suite\n./run-regression-test.sh --run -d demo -xs sql_action\n\n# Exclude the test cases in the demo directory\n./run-regression-test.sh --run -xd demo\n\n# Exclude the test cases in the "demo" group\n./run-regression-test.sh --run -xg demo\n\n# Self-defined configurations\n./run-regression-test.sh --run -conf a=b\n\n# Parallel execution\n./run-regression-test.sh --run -parallel 5 -suiteParallel 10 -actionParallel 20\n')),(0,r.yg)("h2",{id:"generate-out-file-based-on-the-query-result"},"Generate ",(0,r.yg)("inlineCode",{parentName:"h2"},".out")," File Based on the Query Result"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"# Automatically generate an .out file for the sql_action test case based on the query result. Ignore this if the .out file already exists.\n./run-regression-test.sh --run sql_action -genOut\n\n# Automatically generate an .out file for the sql_action test case based on the query result. If an .out file already exist, it will be overwritten.\n./run-regression-test.sh --run sql_action -forceGenOut\n")),(0,r.yg)("h2",{id:"suite-plug-in"},"Suite Plug-in"),(0,r.yg)("p",null,"Sometimes there might be a need to expand the suite classes, but it is not easy to change the source code of suite classes. In this case, you might do that by plug-ins. The default directory for plug-ins is ",(0,r.yg)("inlineCode",{parentName:"p"},"${DORIS_HOME}/regression-test/plugins"),", in whic you can define how to expand suite classes with a groovy script. For example, this is how to add a ",(0,r.yg)("inlineCode",{parentName:"p"},"testPlugin")," function for log printing to ",(0,r.yg)("inlineCode",{parentName:"p"},"plugin_example.groovy"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-groovy"},'import org.apache.doris.regression.suite.Suite\n\n// Register `testPlugin` function to Suite,\n// and invoke in ${DORIS_HOME}/regression-test/suites/demo/test_plugin.groovy\nSuite.metaClass.testPlugin = { String info /* param */ ->\n\n    // Which suite invoke current function?\n    Suite suite = delegate as Suite\n\n    // Function body\n    suite.getLogger().info("Test plugin: suiteName: ${suite.name}, info: ${info}".toString())\n\n    // Optional return value\n    return "OK"\n}\n\nlogger.info("Added \'testPlugin\' function to Suite")\n')),(0,r.yg)("p",null,"After adding the ",(0,r.yg)("inlineCode",{parentName:"p"},"testPlugin")," function, you can use it in a regular test case. For example, in ",(0,r.yg)("inlineCode",{parentName:"p"},"${DORIS_HOME}/regression-test/suites/demo/test_plugin.groovy"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-groovy"},'suite("test_plugin", "demo") {\n    // register testPlugin function in ${DORIS_HOME}/regression-test/plugins/plugin_example.groovy\n    def result = testPlugin("message from suite")\n    assertEquals("OK", result)\n}\n')),(0,r.yg)("h2",{id:"cicd-support"},"CI/CD Support"),(0,r.yg)("h3",{id:"teamcity"},"TeamCity"),(0,r.yg)("p",null,"You can use TeamCity to recognize Service Message via stdout. If you start the regression test framework using the ",(0,r.yg)("inlineCode",{parentName:"p"},"--teamcity")," parameter, the TeamCity Service Message will be printed in stdout. TeamCity will automatically read the event logs in stdout, and show ",(0,r.yg)("inlineCode",{parentName:"p"},"Tests")," in the current pipeline, in which there will be the test and logs. Therefore, you only need to configure the following command to start the regression test framework. In the following snippet, ",(0,r.yg)("inlineCode",{parentName:"p"},"-Dteamcity.enableStdErr=false")," means to print error logs to stdout, too, so that the logs can be chronologically organized."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},'JAVA_OPTS="-Dteamcity.enableStdErr=${enableStdErr}" ./run-regression-test.sh --teamcity --run\n')),(0,r.yg)("h2",{id:"e2e-test-with-external-data-sources"},"E2E Test with External Data Sources"),(0,r.yg)("p",null,"Doris supports queries on external data sources, so the regression testing framework allows users to build external data sources using Docker Compose, so that they can run end-to-end tests with external data sources. "),(0,r.yg)("ol",{start:0},(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Preparation"),(0,r.yg)("p",{parentName:"li"},"To begin with, modify ",(0,r.yg)("inlineCode",{parentName:"p"},"CONTAINER_UID")," in ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/custom_settings.env"),". For example, ",(0,r.yg)("inlineCode",{parentName:"p"},"doris-10002-18sda1-"),". The follow-up startup scripts will replace the corresponding names in Docker Compose to ensure consistency across multiple containers environment.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Start the Container"),(0,r.yg)("p",{parentName:"li"},"So far, Doris has supported Docker Compose for data sources including Elasticsearch, MySQL, PostgreSQL, Hive, SQLServer, Oracle, Iceberg, Hudi, and Trino. The relevant files can be found in the directory ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose"),"."),(0,r.yg)("p",{parentName:"li"},"By default, you can use the following command to start the Docker containers for all external data sources:"),(0,r.yg)("p",{parentName:"li"},"(Note that for Hive and Hudi containers, you also need to download the pre-built data files. See the relevant documentation of Hive and Hudi.)"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},"cd docker/thirdparties && sh run-thirdparties-docker.sh\n")),(0,r.yg)("p",{parentName:"li"},"Executing this command requires root or sudo privilege. If the command returns, that means all containers are started. You can check on them by inputing a ",(0,r.yg)("inlineCode",{parentName:"p"},"docker ps -a")," command. "),(0,r.yg)("p",{parentName:"li"},"To stop all containers, you can use the following command:"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},"cd docker/thirdparties && sh run-thirdparties-docker.sh --stop\n")),(0,r.yg)("p",{parentName:"li"},"To start or stop some specific components, you can use the following commands:"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},"cd docker/thirdparties\n# Start MySQL\nsh run-thirdparties-docker.sh -c mysql\n# Start MySQL, PostgreSQL, Iceberg\nsh run-thirdparties-docker.sh -c mysql,pg,iceberg\n# Stop MySQL, PostgreSQL, Iceberg\nsh run-thirdparties-docker.sh -c mysql,pg,iceberg --stop\n")),(0,r.yg)("ol",{parentName:"li"},(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"MySQL"),(0,r.yg)("p",{parentName:"li"},"You can find the MySQL-related Docker Compose files in ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose/mysql")," ."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"mysql-5.7.yaml.tpl"),": Docker Compose file template, no modification required. The default username and password is root/123456"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"mysql-5.7.env"),": Configuration file, in which you can configure the external port of the MySQL container (default number: 3316)."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"init/"),": SQL files in this directory will be automatically executed once the container is created."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"data/"),": The container will be mounted to this local data directory after it is started. The ",(0,r.yg)("inlineCode",{parentName:"li"},"run-thirdparties-docker.sh"),"  script will clear and rebuild this directory every time it is started."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"PostgreSQL"),(0,r.yg)("p",{parentName:"li"},"You can find the PostgreSQL-related Docker Compose files in  ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose/postgresql")," ."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"postgresql-14.yaml.tpl"),": Docker Compose file template, no modification required. The default username and password is postgres/123456"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"postgresql-14.env"),": Configuration file, in which you can configure the external port of the PostgreSQL container (default number: 5442)."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"init/"),": SQL files in this directory will be automatically executed once the container is created. By default, that includes the creation of database, table, and some data input."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"data/"),": The container will be mounted to this local data directory after it is started. The ",(0,r.yg)("inlineCode",{parentName:"li"},"run-thirdparties-docker.sh"),"  script will clear and rebuild this directory every time it is started."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Hive"),(0,r.yg)("p",{parentName:"li"},"You can find the Hive-related Docker Compose files in ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose/hive")," ."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"p"},"hive-2x.yaml.tpl"),": Docker Compose file template, no modification required.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"p"},"hadoop-hive.env.tpl"),": The configuration file template, no modification required.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"p"},"gen_env.sh"),": The script for initializing the configuration file. In this script, you can modify the two external ports: ",(0,r.yg)("inlineCode",{parentName:"p"},"FS_PORT")," for defaultFs and ",(0,r.yg)("inlineCode",{parentName:"p"},"HMS_PORT")," for Hive metastore (default numbers: 8120 and 9183, respectively). This script will be called once ",(0,r.yg)("inlineCode",{parentName:"p"},"run-thirdparties-docker.sh"),"  is started.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"The ",(0,r.yg)("inlineCode",{parentName:"p"},"scripts/")," directory will be mounted to the container once it is started. Files in this directory require no modifications. Note that you need to download the pre-built files before you start the container: "),(0,r.yg)("p",{parentName:"li"},"Download files from  ",(0,r.yg)("inlineCode",{parentName:"p"},"https://doris-build-hk-1308700295.cos.ap-hongkong.myqcloud.com/regression/load/tpch1_parquet/tpch1.db.tar.gz"),"  to the ",(0,r.yg)("inlineCode",{parentName:"p"},"scripts/")," directory and decompress.")))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Elasticsearch"),(0,r.yg)("p",{parentName:"li"},"You can find three Docker images (for Elasticsearch 6, Elasticsearch 7, and Elasticsearch 8) in ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose/elasticsearch/")," ."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"es.yaml.tpl"),": Docker Compose file template, three versions included, no modifications required."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"es.env"),": Configuration file, in which you need to configure the port number for Elasticsearch. "),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"scripts")," : In this directory, you can find the initialization script after the image is started."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Oracle"),(0,r.yg)("p",{parentName:"li"},"You can find the image for Oracle 11 in ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose/oracle/"),"."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"oracle-11.yaml.tpl"),": Docker Compose file template, no modifications required. "),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"oracle-11.env"),": Configure the external port for Oracle (default number: 1521)."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"SQLServer"),(0,r.yg)("p",{parentName:"li"},"You can find the image for SQLServer 2022 in ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose/sqlserver/")," ."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"sqlserver.yaml.tpl"),": Docker Compose file template, no modifications required. "),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"sqlserver.env"),": Configure the external port for SQLServer (default number: 1433)."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"ClickHouse"),(0,r.yg)("p",{parentName:"li"},"You can find the image for ClickHouse 22 in ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose/clickhouse/"),"."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"clickhouse.yaml.tpl"),": Docker Compose file template, no modifications required."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"clickhouse.env"),": Configure the external port for ClickHouse (default number: 8123)."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Iceberg"),(0,r.yg)("p",{parentName:"li"},"You can find the image of Iceberg + Spark + Minio in ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose/iceberg/"),"."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"iceberg.yaml.tpl"),": Docker Compose file template, no modifications required."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"entrypoint.sh.tpl"),": The template of the initialization script after the image is started, no modifications required."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"spark-defaults.conf.tpl"),": Configuration file template for Spark, no modifications required."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"iceberg.env"),": Configuration file for external ports. You need to modify every external port to avoid conflicts.")),(0,r.yg)("p",{parentName:"li"},"After the image is started, execute the following command to start spark-sql:"),(0,r.yg)("p",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"p"},"docker exec -it doris-xx-spark-iceberg spark-sql"),"        "),(0,r.yg)("p",{parentName:"li"},"In this command, ",(0,r.yg)("inlineCode",{parentName:"p"},"doris-xx-spark-iceberg")," is the container name."),(0,r.yg)("p",{parentName:"li"},"Execution examples for spark-sql iceberg:"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},"create database db1;\nshow databases;\ncreate table db1.test1(k1 bigint, k2 bigint, k3 string) partitioned by (k1);\ninsert into db1.test1 values(1,2,'abc');\nselect * from db1.test1;\nquit;\n")),(0,r.yg)("p",{parentName:"li"},"You can also access by spark-shell:"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},'docker exec -it doris-xx-spark-iceberg spark-shell\n\nspark.sql(s"create database db1")\nspark.sql(s"show databases").show()\nspark.sql(s"create table db1.test1(k1 bigint, k2 bigint, k3 string) partitioned by (k1)").show()\nspark.sql(s"show tables from db1").show()\nspark.sql(s"insert into db1.test1 values(1,2,\'abc\')").show()\nspark.sql(s"select * from db1.test1").show()\n:q\n')),(0,r.yg)("p",{parentName:"li"},"For more usage guide, see  ",(0,r.yg)("a",{parentName:"p",href:"https://tabular.io/blog/docker-spark-and-iceberg/"},"Tabular Documentation"),".")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Hudi"),(0,r.yg)("p",{parentName:"li"},"You can find the Hudi-related Docker Compose file in ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose/hudi"),"."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"p"},"hudi.yaml.tpl"),": Docker Compose file template, no modifications required.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"p"},"hadoop.env"),": Configuration file template, no modifications required.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"The ",(0,r.yg)("inlineCode",{parentName:"p"},"scripts/")," directory will be mounted to the container once it is started. Files in this directory require no modifications. Note that you need to download the pre-built files before you start the container: "),(0,r.yg)("p",{parentName:"li"},"Download files from ",(0,r.yg)("inlineCode",{parentName:"p"},"https://doris-build-hk-1308700295.cos.ap-hongkong.myqcloud.com/regression/load/hudi/hudi_docker_compose_attached_file.zip"),"  to the ",(0,r.yg)("inlineCode",{parentName:"p"},"scripts/")," directory and decompress.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Before starting, add the following configurations to ",(0,r.yg)("inlineCode",{parentName:"p"},"/etc/hosts")," to avoid ",(0,r.yg)("inlineCode",{parentName:"p"},"UnknownHostException"),":"))),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},"127.0.0.1 adhoc-1\n127.0.0.1 adhoc-2\n127.0.0.1 namenode\n127.0.0.1 datanode1\n127.0.0.1 hiveserver\n127.0.0.1 hivemetastore\n127.0.0.1 sparkmaster\n")),(0,r.yg)("p",{parentName:"li"},"After starting, you can execute the following command to start a Hive query:"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},"docker exec -it adhoc-2 /bin/bash\n\nbeeline -u jdbc:hive2://hiveserver:10000 \\\n--hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat \\\n--hiveconf hive.stats.autogather=false\n\nshow tables;\nshow partitions stock_ticks_mor_rt;\nselect symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = 'GOOG';\nselect symbol, max(ts) from stock_ticks_mor_ro group by symbol HAVING symbol = 'GOOG';\nexit;\n")),(0,r.yg)("p",{parentName:"li"},"You can also access by spark-shell:"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre"},'docker exec -it adhoc-1 /bin/bash\n\n$SPARK_INSTALL/bin/spark-shell \\\n  --jars /var/scripts/hudi_docker_compose_attached_file/jar/hoodie-hive-sync-bundle.jar \\\n  --master local[2] \\\n  --driver-class-path $HADOOP_CONF_DIR \\\n  --conf spark.sql.hive.convertMetastoreParquet=false \\\n  --deploy-mode client \\\n  --driver-memory 1G \\\n  --executor-memory 3G \\\n  --num-executors 1\n\nspark.sql("show tables").show(100, false)\nspark.sql("select symbol, max(ts) from stock_ticks_cow group by symbol HAVING symbol = \'GOOG\'").show(100, false)\nspark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_cow where  symbol = \'GOOG\'").show(100, false)\nspark.sql("select symbol, max(ts) from stock_ticks_mor_ro group by symbol HAVING symbol = \'GOOG\'").show(100, false)\nspark.sql("select symbol, max(ts) from stock_ticks_mor_rt group by symbol HAVING symbol = \'GOOG\'").show(100, false)\nspark.sql("select `_hoodie_commit_time`, symbol, ts, volume, open, close  from stock_ticks_mor_ro where  symbol = \'GOOG\'").show(100, false)\n:q\n')),(0,r.yg)("p",{parentName:"li"},"For more usage guide, see ",(0,r.yg)("a",{parentName:"p",href:"https://hudi.apache.org/docs/docker_demo"},"Hudi Documentation"),".")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Trino\nYou can find the Trino-related Docker Compose file in ",(0,r.yg)("inlineCode",{parentName:"p"},"docker/thirdparties/docker-compose/trino"),"."),(0,r.yg)("p",{parentName:"li"},"Template files:"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"gen_env.sh.tpl: This is used to generate HDFS-related port numbers, no modifications required, but you can change the port numbers in the case of port conflicts.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"hive.properties.tpl: This is used to configure the Trino catalog, no modifications required. ")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"trino_hive.env.tpl: Environment configurations of Hive, no modifications required. ")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"trino_hive.yaml.tpl: Docker Compose file, no modifications required. "),(0,r.yg)("p",{parentName:"li"},"After the Trino Docker is started, a Trino + Hive Catalog will be configured, and then Trino will have two catalogs."))),(0,r.yg)("ol",{parentName:"li"},(0,r.yg)("li",{parentName:"ol"},"Hive"),(0,r.yg)("li",{parentName:"ol"},"TPCH (self-contained in Trino Docker)")),(0,r.yg)("p",{parentName:"li"},"For more usage guide, see  ",(0,r.yg)("a",{parentName:"p",href:"https://trino.io/docs/current/installation/containers.html"},"Trino Documentation"),".")))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},"Run the regression test"),(0,r.yg)("p",{parentName:"li"},"Regression test for external data sources is disabled by default. You can enable it by modifying the following configurations in  ",(0,r.yg)("inlineCode",{parentName:"p"},"regression-test/conf/regression-conf.groovy")," :"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"enableJdbcTest"),": This is to enable test for JDBC external tables. For this purpose, you need to start the MySQL and PostgreSQL containers."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"mysql_57_port")," and ",(0,r.yg)("inlineCode",{parentName:"li"},"pg_14_port")," are the external port of MySQL and PostgreSQL, respectively. Default port numbers: 3316 and 5442."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"enableHiveTest"),": This is to enable test for Hive external tables. For this purpose, you need to start the Hive container."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"hms_port")," is the external port for Hive metastore. Default number: 9183."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"enableEsTest"),": This is to enable test for Elasticsearch external tables. For this purpose, you need to start the Elasticsearch container."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"es_6_port"),": Port for Elasticsearch 6."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"es_7_port"),": Port for Elasticsearch 7."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"es_8_port"),": Port for Elasticsearch 8.")))))}d.isMDXComponent=!0}}]);