"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([[951158],{15680:(e,n,t)=>{t.d(n,{xA:()=>g,yg:()=>m});var a=t(296540);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),p=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},g=function(e){var n=p(e.components);return a.createElement(l.Provider,{value:n},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,g=s(e,["components","mdxType","originalType","parentName"]),u=p(t),d=r,m=u["".concat(l,".").concat(d)]||u[d]||c[d]||i;return t?a.createElement(m,o(o({ref:n},g),{},{components:t})):a.createElement(m,o({ref:n},g))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=d;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[u]="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=t[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},124842:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=t(58168),r=(t(296540),t(15680));const i={title:"Apache Doris 2.1.3 just released",description:"This version has updated several improvements, including writing data back to Hive, materialized view, permission management and bug fixes. It further enhances the performance and stability of the system.",date:"2024-05-21",author:"Apache Doris",tags:["Release Notes"],image:"/images/2.1.3.jpg"},o=void 0,s={permalink:"/zh-CN/blog/release-note-2.1.3",source:"@site/blog/release-note-2.1.3.md",title:"Apache Doris 2.1.3 just released",description:"This version has updated several improvements, including writing data back to Hive, materialized view, permission management and bug fixes. It further enhances the performance and stability of the system.",date:"2024-05-21T00:00:00.000Z",formattedDate:"2024\u5e745\u670821\u65e5",tags:[{label:"Release Notes",permalink:"/zh-CN/blog/tags/release-notes"}],hasTruncateMarker:!1,authors:[{name:"Apache Doris"}],frontMatter:{title:"Apache Doris 2.1.3 just released",description:"This version has updated several improvements, including writing data back to Hive, materialized view, permission management and bug fixes. It further enhances the performance and stability of the system.",date:"2024-05-21",author:"Apache Doris",tags:["Release Notes"],image:"/images/2.1.3.jpg"},prevItem:{title:"Apache Doris for log and time series data analysis in NetEase, why not Elasticsearch and InfluxDB?",permalink:"/zh-CN/blog/apache-doris-for-log-and-time-series-data-analysis-in-netease"},nextItem:{title:"Apache Doris version 2.0.10 has been released",permalink:"/zh-CN/blog/release-note-2.0.10"}},l={authorsImageUrls:[void 0]},p=[{value:"New features",id:"new-features",level:2},{value:"Improvements",id:"improvements",level:2},{value:"Behavior changes",id:"behavior-changes",level:2},{value:"Upgrade Issues",id:"upgrade-issues",level:2},{value:"Bug fixes",id:"bug-fixes",level:2}],g={toc:p},u="wrapper";function c(e){let{components:n,...t}=e;return(0,r.yg)(u,(0,a.A)({},g,t,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("p",null,"Apache Doris 2.1.3 was officially released on May 21, 2024. This version has updated several improvements, including writing data back to Hive, materialized view, permission management and bug fixes. It further enhances the performance and stability of the system."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Quick Download:")," ",(0,r.yg)("a",{parentName:"p",href:"https://doris.apache.org/download/"},"https://doris.apache.org/download/")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"GitHub Release:")," ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/apache/doris/releases"},"https://github.com/apache/doris/releases")),(0,r.yg)("h2",{id:"new-features"},"New features"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"1. Support writing back data to hive tables via Hive Catalog")),(0,r.yg)("p",null,"Starting from version 2.1.3, Apache Doris supports DDL and DML operations on Hive. Users can directly create libraries and tables in Hive through Apache Doris and write data to Hive tables by executing ",(0,r.yg)("inlineCode",{parentName:"p"},"INSERT INTO")," statements. This feature allows users to perform complete data query and write operations on Hive through Apache Doris, further simplifying the integrated lakehouse architecture."),(0,r.yg)("p",null,"Please refer: ",(0,r.yg)("a",{parentName:"p",href:"https://doris.apache.org/docs/lakehouse/datalake-building/hive-build/"},"https://doris.apache.org/docs/lakehouse/datalake-building/hive-build/")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"2. Support building new asynchronous materialized views on top of existing ones")),(0,r.yg)("p",null,"Users can create new asynchronous materialized views on top of existing ones, directly reusing pre-computed intermediate results for data processing. This simplifies complex aggregation and computation operations, reducing resource consumption and maintenance costs while further accelerating query performance and improving data availability."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"3. Support rewriting through nested materialized views")),(0,r.yg)("p",null,"Materialized View (MV) is a database object used to store query results. Now, Apache Doris supports rewriting through nested materialized views, which helps optimize query performance."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"4. New ",(0,r.yg)("inlineCode",{parentName:"strong"},"SHOW VIEWS")," statement")),(0,r.yg)("p",null,"The ",(0,r.yg)("inlineCode",{parentName:"p"},"SHOW VIEWS")," statement can be used to query views in the database, facilitating better management and understanding of view objects in the database."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"5. Workload Group supports binding to specific BE nodes")),(0,r.yg)("p",null,"Workload Group can be bound to specific BE nodes, enabling more refined control over query execution to optimize resource usage and improve performance."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"6. Broker Load supports compressed JSON format")),(0,r.yg)("p",null,"Broker Load now supports importing compressed JSON format data, significantly reducing bandwidth requirements for data transmission and accelerating data import performance."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"7. TRUNCATE Function can use columns as scale parameters")),(0,r.yg)("p",null,"The TRUNCATE function can now accept columns as scale parameters, providing more flexibility when processing numerical data."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"8. Add new functions ",(0,r.yg)("inlineCode",{parentName:"strong"},"uuid_to_int")," and ",(0,r.yg)("inlineCode",{parentName:"strong"},"int_to_uuid"))),(0,r.yg)("p",null,"These two functions allow users to convert between UUID and integer, significantly helping in scenarios that require handling UUID data."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"9. Add ",(0,r.yg)("inlineCode",{parentName:"strong"},"bypass_workload_group")," session variable to bypass query queue")),(0,r.yg)("p",null,"The ",(0,r.yg)("inlineCode",{parentName:"p"},"bypass_workload_group")," session variable allows certain queries to bypass the Workload Group queue and execute directly, which is useful for handling critical queries that require quick responses."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"10. Add strcmp function")),(0,r.yg)("p",null,"The strcmp function compares two strings and returns their comparison result, simplifying text data processing."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"11. Support HLL functions ",(0,r.yg)("inlineCode",{parentName:"strong"},"hll_from_base64")," and ",(0,r.yg)("inlineCode",{parentName:"strong"},"hll_to_base64"))),(0,r.yg)("p",null,"HyperLogLog (HLL) is an algorithm for cardinality estimation. These two functions allow users to decode HLL data from a Base64-encoded string or encode HLL data as a Base64 string, which is very useful for storing and transmitting HLL data."),(0,r.yg)("h2",{id:"improvements"},"Improvements"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"1. Replace SipHash with XXHash to improve shuffle performance")),(0,r.yg)("p",null,"Both SipHash and XXHash are hashing functions, but XXHash may provide faster hashing speeds and better performance in certain scenarios. This optimization aims to improve performance during data shuffling by adopting XXHash."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"2. Asynchronous materialized views support NULL partition columns in OLAP tables")),(0,r.yg)("p",null,"This enhancement allows asynchronous materialized views to support NULL partition columns in OLAP tables, enhancing data processing flexibility."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"3. Limit maximum string length to 1024 when collecting column statistics to control BE memory usage")),(0,r.yg)("p",null,"Limiting the string length when collecting column statistics prevents excessive data from consuming too much BE memory, helping maintain system stability and performance."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"4. Support dynamic deletion of Bitmap cache to improve performance")),(0,r.yg)("p",null,"Dynamically deleting no longer needed Bitmap Cache can free up memory and improve system performance."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"5. Reduce memory usage during ALTER operations")),(0,r.yg)("p",null,"Reducing memory usage during ALTER operations improves the efficiency of system resource utilization."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"6. Support constant folding for complex types")),(0,r.yg)("p",null,"Supports constant folding for Array/Map/Struct complex types."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"7. Add support for Variant type in Aggregate Key Model")),(0,r.yg)("p",null,"The Variant data type can store multiple data types. This optimization allows aggregation operations on Variant type data, enhancing the flexibility of semi-structured data analysis."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"8. Support new inverted index format in CCR")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"9. Optimize rewriting performance for nested materialized views")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"10. Support decimal256 type in row-based storage format")),(0,r.yg)("p",null,"Supporting the decimal256 type in row-based storage extends the system's ability to handle high-precision numerical data."),(0,r.yg)("h2",{id:"behavior-changes"},"Behavior changes"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"1. Authorization")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Grant_priv permission changes"),": ",(0,r.yg)("inlineCode",{parentName:"p"},"Grant_priv")," can no longer be arbitrarily granted. When performing a ",(0,r.yg)("inlineCode",{parentName:"p"},"GRANT")," operation, the user not only needs to have ",(0,r.yg)("inlineCode",{parentName:"p"},"Grant_priv")," but also the permissions to be granted. For example, to grant ",(0,r.yg)("inlineCode",{parentName:"p"},"SELECT")," permission on ",(0,r.yg)("inlineCode",{parentName:"p"},"table1"),", the user needs both ",(0,r.yg)("inlineCode",{parentName:"p"},"GRANT")," permission and ",(0,r.yg)("inlineCode",{parentName:"p"},"SELECT")," permission on ",(0,r.yg)("inlineCode",{parentName:"p"},"table1"),", enhancing security and consistency in permission management.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Workload group and resource usage_priv"),": ",(0,r.yg)("inlineCode",{parentName:"p"},"Usage_priv")," for Workload Group and Resource is no longer global but limited to Resource and Workload Group, making permission granting and usage more specific.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Authorization for operations"),": Operations that were previously unauthorized now have corresponding authorizations for more detailed and comprehensive operational permission control."))),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"2. LOG directory configuration")),(0,r.yg)("p",null,"The log directory configuration for FE and BE now uniformly uses the ",(0,r.yg)("inlineCode",{parentName:"p"},"LOG_DIR")," environment variable. All other different types of logs will be stored with ",(0,r.yg)("inlineCode",{parentName:"p"},"LOG_DIR")," as the root directory. To maintain compatibility between versions, the previous configuration item ",(0,r.yg)("inlineCode",{parentName:"p"},"sys_log_dir")," can still be used."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"3. S3 Table Function (TVF)")),(0,r.yg)("p",null,"Due to issues with correctly recognizing or processing S3 URLs in certain cases, the parsing logic for object storage paths has been refactored. For file paths in S3 table functions, the ",(0,r.yg)("inlineCode",{parentName:"p"},"force_parsing_by_standard_uri")," parameter needs to be passed to ensure correct parsing."),(0,r.yg)("h2",{id:"upgrade-issues"},"Upgrade Issues"),(0,r.yg)("p",null,"Since many users use certain keywords as column names or attribute values, the following keywords have been set as non-reserved, allowing users to use them as identifiers."),(0,r.yg)("h2",{id:"bug-fixes"},"Bug fixes"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"1. Fix no data error when reading Hive tables on Tencent Cloud COSN")),(0,r.yg)("p",null,"Resolved the no data error that could occur when reading Hive tables on Tencent Cloud COSN, enhancing compatibility with Tencent Cloud storage services."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"2. Fix incorrect results returned by ",(0,r.yg)("inlineCode",{parentName:"strong"},"milliseconds_diff")," function")),(0,r.yg)("p",null,"Fixed an issue where the ",(0,r.yg)("inlineCode",{parentName:"p"},"milliseconds_diff")," function returned incorrect results in some cases, ensuring the accuracy of time difference calculations."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"3. User-defined variables should be rorwarded to the Master node")),(0,r.yg)("p",null,"Ensured that user-defined variables are correctly passed to the Master node for consistency and correct execution logic across the entire system."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"4. Fix Schema Change issues when adding complex type columns")),(0,r.yg)("p",null,"Resolved Schema Change issues that could arise when adding complex type columns, ensuring the correctness of Schema Changes."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"5. Fix data loss issue in Routine Load when FE Master node changes")),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"Routine Load")," is often used to subscribe to Kafka message queues. This fix addresses potential data loss issues that may occur during FE Master node changes."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"6. Fix Routine Load failure when Workload Group cannot be found")),(0,r.yg)("p",null,"Resolved an issue where ",(0,r.yg)("inlineCode",{parentName:"p"},"Routine Load")," would fail if the specified Workload Group could not be found."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"7. Support column string64 to avoid join failures when string size overflows unit32")),(0,r.yg)("p",null,"In some cases, string sizes may exceed the unit32 limit. Supporting the ",(0,r.yg)("inlineCode",{parentName:"p"},"string64")," type ensures correct execution of string JOIN operations."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"8. Allow Hadoop users to create Paimon Catalog")),(0,r.yg)("p",null,"Permitted authorized Hadoop users to create Paimon Catalogs."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"9. Fix ",(0,r.yg)("inlineCode",{parentName:"strong"},"function_ipxx_cidr")," function issues with constant parameters")),(0,r.yg)("p",null,"Resolved problems with the ",(0,r.yg)("inlineCode",{parentName:"p"},"function_ipxx_cidr")," function when handling constant parameters, ensuring the correctness of function execution."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"10. Fix file download errors when restoring using HDFS")),(0,r.yg)("p",null,'Resolved "failed to download" errors encountered during data restoration using HDFS, ensuring the accuracy and reliability of data recovery.'),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"11. Fix column permission issues related to hidden columns")),(0,r.yg)("p",null,"In some cases, permission settings for hidden columns may be incorrect. This fix ensures the correctness and security of column permission settings."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"12. Fix issue where Arrow Flight cannot obtain the correct IP in K8s deployments")),(0,r.yg)("p",null,"This fix resolves an issue where Arrow Flight cannot correctly obtain the IP address in Kubernetes deployment environments."))}c.isMDXComponent=!0}}]);